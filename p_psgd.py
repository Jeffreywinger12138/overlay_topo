# -*- coding: utf-8 -*-
"""P-PSGD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wmuC1Wsrz-FvGtAkFkPDrgc-uB_HCaWr
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
import numpy as np

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the data
y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)  # Ensure labels are integers

# Split training data into training and validation sets
x_train, x_val = x_train[:50000], x_train[50000:]
y_train, y_val = y_train[:50000], y_train[50000:]

# Define the model architecture
def create_model():
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation='relu'),
        Dense(10)
    ])
    return model

# Initialize models and optimizers for each agent
num_agents = 5
agents_models = [create_model() for _ in range(num_agents)]
optimizers = [tf.keras.optimizers.SGD(learning_rate=0.01) for _ in range(num_agents)]

# Define the mixing matrix W
W = np.ones((num_agents, num_agents)) / num_agents

# Training parameters
batch_size = 64
epochs = 20

# Define loss function
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# Example of tracking accuracy
lost_hitory = []
val_accuracies = []
# Custom training loop
for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")
    # Reset epoch loss for each agent
    epoch_loss = [0] * num_agents
    # Shuffle and batch the data
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)
    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(2000).batch(batch_size)
    for step, (x_batch, y_batch) in enumerate(dataset):
        for i in range(num_agents):
            # Aggregation of parameters based on W
            aggregated_params = []
            for var_idx, _ in enumerate(agents_models[i].trainable_variables):
                weighted_sum = sum(W[i, j] * agents_models[j].trainable_variables[var_idx] for j in range(num_agents))
                aggregated_params.append(weighted_sum)

            # Applying the gradient descent step
            with tf.GradientTape() as tape:
                # Compute loss using the original parameters
                predictions = agents_models[i](x_batch, training=True)
                loss = loss_fn(y_batch, predictions)
            # Update epoch loss for agent i
            epoch_loss[i] += loss.numpy()
            # Compute gradients with respect to the original parameters
            grads = tape.gradient(loss, agents_models[i].trainable_variables)

            # update model parameters to the aggregated ones for gradient update
            for var, new_val in zip(agents_models[i].trainable_variables, aggregated_params):
                var.assign(new_val)
            # Update parameters using the computed gradients
            optimizers[i].apply_gradients(zip(grads, agents_models[i].trainable_variables))

            if step % 100 == 0:
                print(f"Step {step}: Loss = {loss.numpy()}")
    lost_hitory.append(sum(epoch_loss)/len(epoch_loss))
    print(f"Epoch {epoch+1}, global objective loss: {sum(epoch_loss)/len(epoch_loss)}")
    # Validation step at the end of each epoch
    total_correct = 0
    total_val_samples = 0
    for x_batch_val, y_batch_val in val_dataset:
        val_predictions = [agents_models[i](x_batch_val, training=False) for i in range(num_agents)]
        # Assuming a strategy to aggregate predictions from all agents, e.g., averaging
        avg_predictions = tf.reduce_mean(val_predictions, axis=0)
        correct_preds = tf.equal(tf.argmax(avg_predictions, axis=1, output_type=tf.int32), y_batch_val)
        total_correct += tf.reduce_sum(tf.cast(correct_preds, tf.float32))
        total_val_samples += x_batch_val.shape[0]
    val_accuracy = total_correct / total_val_samples
    val_accuracies.append(val_accuracy)
    print(f"Epoch {epoch+1}, Validation Accuracy: {val_accuracy.numpy()}")

import matplotlib.pyplot as plt

epochs = range(1, len(lost_hitory) + 1)

plt.figure(figsize=(10, 5))

# Plotting loss history
plt.subplot(1, 2, 1)
plt.plot(epochs, lost_hitory, label='Loss')
plt.title('Global Average Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plotting validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='orange')
plt.title('Validation Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

def evaluate_model(model, dataset):
    """
    Evaluate the model on a given dataset.

    Parameters:
    - model: TensorFlow model to evaluate.
    - dataset: tf.data.Dataset containing (features, labels) pairs.

    Returns:
    - mean_loss: The average loss over the dataset.
    - accuracy: The accuracy of the model over the dataset.
    """
    # Metrics to accumulate
    total_loss = 0
    correct_predictions = 0
    num_samples = 0

    for x_batch, y_batch in dataset:
        predictions = model(x_batch, training=False)
        loss = loss_fn(y_batch, predictions)

        # Update metrics
        total_loss += loss.numpy() * x_batch.shape[0]
        correct_predictions += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.cast(y_batch, tf.int64)), tf.float32)).numpy()
        num_samples += x_batch.shape[0]

    mean_loss = total_loss / num_samples
    accuracy = correct_predictions / num_samples

    return mean_loss, accuracy

# Evaluate each agent's model using the custom evaluation function

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)
for i, model in enumerate(agents_models):
    mean_loss, test_accuracy = evaluate_model(model, test_dataset)
    print(f"Agent {i+1} - Test Loss: {mean_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")